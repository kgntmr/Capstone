{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection ðŸ› \n",
    "\n",
    "The subjQA dataset is constructed based on publicly available review datasets. Specifically, the movies, books, electronics, and grocery categories are constructed using reviews from the Amazon Review dataset. The TripAdvisor category, as the name suggests, is constructed using reviews from TripAdvisor which can be found [here](link). Finally, the restaurants category is constructed using the Yelp Dataset which is also publicly available.\n",
    "\n",
    "The process of constructing SubjQA is discussed in detail in our paper. In a nutshell, the dataset construction consists of the following steps:\n",
    "\n",
    "1. First, all opinions expressed in reviews are extracted. In the pipeline, each opinion is modeled as a (modifier, aspect) pair which is a pair of spans where the former describes the latter. *(e.g., \"good, hotel\", and \"terrible, acting\" are a few examples of extracted opinions)*.\n",
    "2. Using Matrix Factorization techniques, implication relationships between different expressed opinions are mined. For instance, the system mines that \"responsive keys\" implies \"good keyboard\". In our pipeline, we refer to the conclusion of an implication (i.e., \"good keyboard\" in this example) as the query opinion, and we refer to the premise (i.e., \"responsive keys\") as its neighboring opinion.\n",
    "3. Annotators are then asked to write a question based on query opinions. For instance, given \"good keyboard\" as the query opinion, they might write \"Is this keyboard any good?\"\n",
    "4. Each question written based on a query opinion is then paired with a review that mentions its neighboring opinion. In our example, that would be a review that mentions \"responsive keys\".\n",
    "5. The question and review pairs are presented to annotators to select the correct answer span, and rate the subjectivity level of the question as well as the subjectivity level of the highlighted answer span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format ðŸ“Š\n",
    "\n",
    "All files are in standard CSV format, and they consist of the following columns:\n",
    "\n",
    "- **domain**: The category/domain of the review (e.g., hotels, books, ...).\n",
    "- **question**: The question (written based on a query opinion).\n",
    "- **review**: The review (that mentions the neighboring opinion).\n",
    "- **human_ans_spans**: The span labeled by annotators as the answer.\n",
    "- **human_ans_indices**: The (character-level) start and end indices of the answer span highlighted by annotators.\n",
    "- **question_subj_level**: The subjectivity level of the question (on a 1 to 5 scale with 1 being the most subjective).\n",
    "- **ques_subj_score**: The subjectivity score of the question computed using the TextBlob package.\n",
    "- **is_ques_subjective**: A boolean subjectivity label derived from question_subj_level (i.e., scores below 4 are considered as subjective).\n",
    "- **answer_subj_level**: The subjectivity level of the answer span (on a 1 to 5 scale with 5 being the most subjective).\n",
    "- **ans_subj_score**: The subjectivity score of the answer span computed using the TextBlob package.\n",
    "- **is_ans_subjective**: A boolean subjectivity label derived from answer_subj_level (i.e., scores below 4 are considered as subjective).\n",
    "- **nn_mod**: The modifier of the neighboring opinion (which appears in the review).\n",
    "- **nn_asp**: The aspect of the neighboring opinion (which appears in the review).\n",
    "- **query_mod**: The modifier of the query opinion (around which a question is manually written).\n",
    "- **query_asp**: The aspect of the query opinion (around which a question is manually written).\n",
    "- **item_id**: The id of the item/business discussed in the review.\n",
    "- **review_id**: A unique id associated with the review.\n",
    "- **q_review_id**: A unique id assigned to the question-review pair.\n",
    "- **q_reviews_id**: A unique id assigned to all question-review pairs with a shared question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "Johannes Bjerva, Nikita Bhutani, Behzad Golahn, Wang-Chiew Tan, and Isabelle Augenstein. (2020). SubjQA: A Dataset for Subjectivity and Review Comprehension. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing*. Association for Computational Linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Visualization Libraries\n",
    "import plotly.express as px\n",
    "\n",
    "# Deep Learning and Computation\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine Learning Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data Loading and Acceleration Utilities\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from accelerate import Accelerator\n",
    "import evaluate\n",
    "\n",
    "# Transformers Library for NLP\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A fast tokenizer is optimized for speed and efficiency in tokenizing text\n",
    "# Often implement faster processing, useful for large-scale NLP tasks.\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('subjqa-train.csv')\n",
    "df_test = pd.read_csv('subjqa-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum length and stride parameters for tokenization\n",
    "max_length = 384  # Maximum length of tokenized sequences, commonly used for a balance between context and memory usage\n",
    "stride = 128  # Stride determines overlap between tokenized sequences, providing context while avoiding redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>nn_mod</th>\n",
       "      <th>nn_asp</th>\n",
       "      <th>query_mod</th>\n",
       "      <th>query_asp</th>\n",
       "      <th>q_review_id</th>\n",
       "      <th>q_reviews_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_subj_level</th>\n",
       "      <th>ques_subj_score</th>\n",
       "      <th>is_ques_subjective</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review</th>\n",
       "      <th>human_ans_spans</th>\n",
       "      <th>human_ans_indices</th>\n",
       "      <th>answer_subj_level</th>\n",
       "      <th>ans_subj_score</th>\n",
       "      <th>is_ans_subjective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00BVMXBDO</td>\n",
       "      <td>movies</td>\n",
       "      <td>addictive</td>\n",
       "      <td>show</td>\n",
       "      <td>full</td>\n",
       "      <td>series</td>\n",
       "      <td>d9a9615d45df2f6e6108db4ca46bfded</td>\n",
       "      <td>399f1046fe6bd97990107f9d7aa86f4a</td>\n",
       "      <td>Who is the author of this series?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>090671369dddfeb02db9bf7125a47c79</td>\n",
       "      <td>Whether it be in her portrayal of a nerdy lesb...</td>\n",
       "      <td>ANSWERNOTFOUND</td>\n",
       "      <td>(251, 265)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1404918051</td>\n",
       "      <td>movies</td>\n",
       "      <td>enough simple</td>\n",
       "      <td>film</td>\n",
       "      <td>charming</td>\n",
       "      <td>movie</td>\n",
       "      <td>06ffe37a8023636a3ce00b020a517e87</td>\n",
       "      <td>42d9dd5b0c67150cac1e13308811cbb5</td>\n",
       "      <td>Can we enjoy the movie along with our family ?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>a29821121e74d319cb93f77101e99c88</td>\n",
       "      <td>An outstanding romantic comedy, 13 Going on 30...</td>\n",
       "      <td>ANSWERNOTFOUND</td>\n",
       "      <td>(1195, 1209)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000633ZP</td>\n",
       "      <td>movies</td>\n",
       "      <td>weak</td>\n",
       "      <td>plot</td>\n",
       "      <td>bad</td>\n",
       "      <td>one</td>\n",
       "      <td>3b625c68e91b9e6987a08b84a9a9d234</td>\n",
       "      <td>32d06ccf2132cda644aea791fa688c53</td>\n",
       "      <td>Does this one good?</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>12a1b821f761bd19a75be7b16cef4a7c</td>\n",
       "      <td>To let the truth be known, I watched this movi...</td>\n",
       "      <td>ANSWERNOTFOUND</td>\n",
       "      <td>(1476, 1490)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000AQS0F</td>\n",
       "      <td>movies</td>\n",
       "      <td>outstanding</td>\n",
       "      <td>show</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>series</td>\n",
       "      <td>f3abfa98b011127e7cb49bcd07f8deeb</td>\n",
       "      <td>e546636f0bb9f93d5f24b4ade9ebab45</td>\n",
       "      <td>Is this series good and excelent?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>cd0f92322e67cc9d70de6674caace78c</td>\n",
       "      <td>At the time of my review, there had been 910 c...</td>\n",
       "      <td>this show is OUTSTANDING</td>\n",
       "      <td>(296, 320)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B003Y5H5FG</td>\n",
       "      <td>movies</td>\n",
       "      <td>great</td>\n",
       "      <td>production design</td>\n",
       "      <td>great</td>\n",
       "      <td>costume design</td>\n",
       "      <td>1b03744e764b257592c2c768345c14bc</td>\n",
       "      <td>a0a97e460a194bcb3286fe68d20aadc2</td>\n",
       "      <td>How is the costume design?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>f6b5024393ebc70287befdaf47a50b75</td>\n",
       "      <td>\"Fright Night\" is great! This is how the story...</td>\n",
       "      <td>The costume design by Susan Matheson is great</td>\n",
       "      <td>(1254, 1299)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id  domain         nn_mod             nn_asp  query_mod  \\\n",
       "0  B00BVMXBDO  movies      addictive               show       full   \n",
       "1  1404918051  movies  enough simple               film   charming   \n",
       "2  B0000633ZP  movies           weak               plot        bad   \n",
       "3  B0000AQS0F  movies    outstanding               show  wonderful   \n",
       "4  B003Y5H5FG  movies          great  production design      great   \n",
       "\n",
       "        query_asp                       q_review_id  \\\n",
       "0          series  d9a9615d45df2f6e6108db4ca46bfded   \n",
       "1           movie  06ffe37a8023636a3ce00b020a517e87   \n",
       "2             one  3b625c68e91b9e6987a08b84a9a9d234   \n",
       "3          series  f3abfa98b011127e7cb49bcd07f8deeb   \n",
       "4  costume design  1b03744e764b257592c2c768345c14bc   \n",
       "\n",
       "                       q_reviews_id  \\\n",
       "0  399f1046fe6bd97990107f9d7aa86f4a   \n",
       "1  42d9dd5b0c67150cac1e13308811cbb5   \n",
       "2  32d06ccf2132cda644aea791fa688c53   \n",
       "3  e546636f0bb9f93d5f24b4ade9ebab45   \n",
       "4  a0a97e460a194bcb3286fe68d20aadc2   \n",
       "\n",
       "                                         question  question_subj_level  \\\n",
       "0               Who is the author of this series?                    1   \n",
       "1  Can we enjoy the movie along with our family ?                    1   \n",
       "2                             Does this one good?                    5   \n",
       "3               Is this series good and excelent?                    1   \n",
       "4                      How is the costume design?                    1   \n",
       "\n",
       "   ques_subj_score  is_ques_subjective                         review_id  \\\n",
       "0              0.0               False  090671369dddfeb02db9bf7125a47c79   \n",
       "1              0.5               False  a29821121e74d319cb93f77101e99c88   \n",
       "2              0.6                True  12a1b821f761bd19a75be7b16cef4a7c   \n",
       "3              0.6                True  cd0f92322e67cc9d70de6674caace78c   \n",
       "4              0.0               False  f6b5024393ebc70287befdaf47a50b75   \n",
       "\n",
       "                                              review  \\\n",
       "0  Whether it be in her portrayal of a nerdy lesb...   \n",
       "1  An outstanding romantic comedy, 13 Going on 30...   \n",
       "2  To let the truth be known, I watched this movi...   \n",
       "3  At the time of my review, there had been 910 c...   \n",
       "4  \"Fright Night\" is great! This is how the story...   \n",
       "\n",
       "                                 human_ans_spans human_ans_indices  \\\n",
       "0                                 ANSWERNOTFOUND        (251, 265)   \n",
       "1                                 ANSWERNOTFOUND      (1195, 1209)   \n",
       "2                                 ANSWERNOTFOUND      (1476, 1490)   \n",
       "3                       this show is OUTSTANDING        (296, 320)   \n",
       "4  The costume design by Susan Matheson is great      (1254, 1299)   \n",
       "\n",
       "   answer_subj_level  ans_subj_score  is_ans_subjective  \n",
       "0                  1           0.000              False  \n",
       "1                  1           0.000              False  \n",
       "2                  5           0.000              False  \n",
       "3                  1           0.875               True  \n",
       "4                  1           0.750               True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2501 entries, 0 to 2500\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   item_id              2501 non-null   object \n",
      " 1   domain               2501 non-null   object \n",
      " 2   nn_mod               2501 non-null   object \n",
      " 3   nn_asp               2501 non-null   object \n",
      " 4   query_mod            2501 non-null   object \n",
      " 5   query_asp            2501 non-null   object \n",
      " 6   q_review_id          2501 non-null   object \n",
      " 7   q_reviews_id         2501 non-null   object \n",
      " 8   question             2501 non-null   object \n",
      " 9   question_subj_level  2501 non-null   int64  \n",
      " 10  ques_subj_score      2501 non-null   float64\n",
      " 11  is_ques_subjective   2501 non-null   bool   \n",
      " 12  review_id            2501 non-null   object \n",
      " 13  review               2501 non-null   object \n",
      " 14  human_ans_spans      2501 non-null   object \n",
      " 15  human_ans_indices    2501 non-null   object \n",
      " 16  answer_subj_level    2501 non-null   int64  \n",
      " 17  ans_subj_score       2501 non-null   float64\n",
      " 18  is_ans_subjective    2501 non-null   bool   \n",
      "dtypes: bool(2), float64(2), int64(2), object(13)\n",
      "memory usage: 337.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'domain', 'nn_mod', 'nn_asp', 'query_mod', 'query_asp',\n",
       "       'q_review_id', 'q_reviews_id', 'question', 'question_subj_level',\n",
       "       'ques_subj_score', 'is_ques_subjective', 'review_id', 'review',\n",
       "       'human_ans_spans', 'human_ans_indices', 'answer_subj_level',\n",
       "       'ans_subj_score', 'is_ans_subjective'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2501 entries, 0 to 2500\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   item_id              2501 non-null   object \n",
      " 1   domain               2501 non-null   object \n",
      " 2   nn_mod               2501 non-null   object \n",
      " 3   nn_asp               2501 non-null   object \n",
      " 4   query_mod            2501 non-null   object \n",
      " 5   query_asp            2501 non-null   object \n",
      " 6   q_review_id          2501 non-null   object \n",
      " 7   q_reviews_id         2501 non-null   object \n",
      " 8   question             2501 non-null   object \n",
      " 9   question_subj_level  2501 non-null   int64  \n",
      " 10  ques_subj_score      2501 non-null   float64\n",
      " 11  is_ques_subjective   2501 non-null   bool   \n",
      " 12  review_id            2501 non-null   object \n",
      " 13  review               2501 non-null   object \n",
      " 14  human_ans_spans      2501 non-null   object \n",
      " 15  human_ans_indices    2501 non-null   object \n",
      " 16  answer_subj_level    2501 non-null   int64  \n",
      " 17  ans_subj_score       2501 non-null   float64\n",
      " 18  is_ans_subjective    2501 non-null   bool   \n",
      "dtypes: bool(2), float64(2), int64(2), object(13)\n",
      "memory usage: 337.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the questions and answers\n",
    "- Let's check the questions and answer according to the 'human_ans_indices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the author of this series?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Whether it be in her portrayal of a nerdy lesbian or a punk rock rebel, Maslany's plural personalities, (though very stereotypical), are entertaining eye-candy. Combined with a complex and unpredictable plot line, this show is surprisingly addictive. ANSWERNOTFOUND\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(251, 265)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].human_ans_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANSWERNOTFOUND'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].review[251:265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking the necessary columns for further analysis\n",
    "df_train=df_train[['question','human_ans_indices','review','human_ans_spans']]\n",
    "df_test=df_test[['question','human_ans_indices','review','human_ans_spans']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence evenly spaced numbers\n",
    "df_train['id']=np.linspace(0,len(df_train)-1,len(df_train))\n",
    "df_test['id']=np.linspace(0,len(df_test)-1,len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['id']=df_train['id'].astype(str)\n",
    "df_test['id']=df_test['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2501 entries, 0 to 2500\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   question           2501 non-null   object\n",
      " 1   human_ans_indices  2501 non-null   object\n",
      " 2   review             2501 non-null   object\n",
      " 3   human_ans_spans    2501 non-null   object\n",
      " 4   id                 2501 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 97.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 582 entries, 0 to 581\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   question           582 non-null    object\n",
      " 1   human_ans_indices  582 non-null    object\n",
      " 2   review             582 non-null    object\n",
      " 3   human_ans_spans    582 non-null    object\n",
      " 4   id                 582 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 22.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(df_train.iloc[0].human_ans_indices.split('(')[1].split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(df_train.iloc[0].human_ans_indices.split('(')[1].split(',')[1].split(' ')[1].split(')')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicating where the answers are\n",
    "df_train['answers']=df_train['human_ans_spans']\n",
    "# Actual answer text itself, right answer where should be\n",
    "df_test['answers']=df_test['human_ans_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract answer data and adds it to a new column\n",
    "for i in range(0,len(df_train)):\n",
    "  answer1={}\n",
    "  si=int(df_train.iloc[i].human_ans_indices.split('(')[1].split(',')[0])\n",
    "  ei=int(df_train.iloc[i].human_ans_indices.split('(')[1].split(',')[1].split(' ')[1].split(')')[0])\n",
    "  answer1['text']=[df_train.iloc[i].review[si:ei]]\n",
    "  answer1['answer_start']=[si]\n",
    "  df_train.at[i, 'answers']=answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['ANSWERNOTFOUND'], 'answer_start': [801]} ANSWERNOTFOUND\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[i].answers,df_train.iloc[i].human_ans_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'human_ans_indices', 'review', 'human_ans_spans', 'id',\n",
       "       'answers'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns=['question', 'human_ans_indices', 'context', 'human_ans_spans', 'id',\n",
    "       'answers']\n",
    "\n",
    "df_test.columns=['question', 'human_ans_indices', 'context', 'human_ans_spans','id',\n",
    "       'answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset2 = datasets.Dataset.from_pandas(df_test)\n",
    "train_dataset2 = datasets.Dataset.from_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dfe047f96545a5b5e5fa8d9719e1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2501 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2501, 4862)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the training examples .map() function on training dataset with the preprocessing function\n",
    "train_dataset = train_dataset2.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset2.column_names,\n",
    ")\n",
    "len(train_dataset2), len(train_dataset) # compare the lengths of the original dataset (train_dataset2) and the preprocessed dataset (train_dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that all 2501 examples were processed in 10 seconds at a speed of 260.48 examples per second. The resulting dataset has 4862 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    # Cleaning the questions by stripping leading and trailing whitespace for consistency\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenization; converting questions and contexts into numerical IDs, enabling the model to understand\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length, # Total length of the input sequence\n",
    "        truncation=\"only_second\", # If the total length exceeds max_length, only the context will be truncated\n",
    "        stride=stride, # Overlap between the chunks\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Extracting overflow_to_sample_mapping\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    # Looping over the tokenized inputs\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx]) # Retrieving example IDs\n",
    "\n",
    "        # Adjusting offset mapping based on sequence IDs\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    # Adding example IDs to the tokenized inputs\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f895b1d1fb4097bb6f8fec757c815e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the validation dataset by applying the preprocess_validation_examples function to each example\n",
    "validation_dataset = val_dataset2.map(\n",
    "    preprocess_validation_examples,  # Function to preprocess each example\n",
    "    batched=True,  # Process examples in batches for efficiency\n",
    "    remove_columns=val_dataset2.column_names,  # Remove unnecessary columns from the dataset\n",
    ")\n",
    "\n",
    "# Calculate the length of the preprocessed validation dataset\n",
    "len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 1104\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    # Initialize a defaultdict to map example IDs to their corresponding feature indices\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    # List to store the formatted predicted answers\n",
    "    predicted_answers = []\n",
    "\n",
    "    # Placeholder values for n_best and max_answer_length\n",
    "    n_best = 20\n",
    "    max_answer_length = 30 \n",
    "\n",
    "    # Process each example to generate predictions\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Iterate through all features linked to the current example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Determine top n_best start and end positions\n",
    "            start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "\n",
    "            # Generate candidate answers based on top start/end positions\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Validate answer positions\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    # Formulate the answer and score\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Choose the best answer for the current example\n",
    "        if answers:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    # Correctly format the references from the examples dataset\n",
    "    references_corrected = []\n",
    "    for ex in examples:\n",
    "        # Split answers if needed and create the correct format\n",
    "        individual_answers = [{'text': ans, 'answer_start': 0} for ans in ex['answers'].split('|')]\n",
    "        references_corrected.append({\n",
    "            'id': ex['id'],\n",
    "            'answers': individual_answers\n",
    "        })\n",
    "\n",
    "    # Compute the evaluation metric using the formatted predictions and references\n",
    "    return metric.compute(predictions=predicted_answers, references=references_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForQuestionAnswering.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameMetricsLogger(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        # Initialize an empty list to store metrics dictionaries\n",
    "        self.metrics_list = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # This method is called whenever logs are emitted by the Trainer\n",
    "        if logs is not None:\n",
    "            # Append the log metrics directly to the list\n",
    "            self.metrics_list.append(logs.copy())  # Copy to ensure no overwriting\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        return pd.DataFrame(self.metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"roberta-finetuned-subjqa-for-rag\",\n",
    "    evaluation_strategy=\"epoch\",                    # Evaluate at the end of each epoch\n",
    "    logging_strategy=\"steps\",                       # Log every specified number of steps\n",
    "    logging_steps=9,                                # Number of steps to log after\n",
    "    logging_dir='./logs',                           # Directory where logs will be saved\n",
    "    save_strategy=\"epoch\",                          # Save the model at the end of each epoch\n",
    "    learning_rate=2e-5,                             # Learning rate\n",
    "    num_train_epochs=7,                          \n",
    "    weight_decay=0.01,                              # Weight decay for regularization\n",
    "    push_to_hub=False,                              # Whether to push the model to the Hugging Face Hub\n",
    "    report_to=\"all\",                                # Reporting to all integrations\n",
    "    fp16=False                                      # Disable mixed precision training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataFrame logger\n",
    "dataframe_logger = DataFrameMetricsLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aishwarya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup the trainer with the logger\n",
    "trainer = Trainer(\n",
    "    model=model1,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[dataframe_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76149b8c7d7c4b9995f1adb6af0d55a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, val_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logged metrics to DataFrame\n",
    "metrics_df = dataframe_logger.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df[['loss', 'grad_norm', 'learning_rate', 'epoch']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Loss:*** Represents the loss function value calculated at a given epoch.\n",
    "- ***grad_norm:*** Refers to the norm (magnitude) of the gradient vector. This is an important metric for understanding the stability of the training process. High gradient norms can indicate issues like exploding gradients.\n",
    "- ***learning_rate:*** Shows the learning rate at each epoch. This parameter controls the size of the steps the optimizer takes while updating the weights. Changes in the learning rate can significantly affect model training dynamics.\n",
    "- ***Epoch:*** Indicates the progression of the training process in terms of epochs. An epoch represents one complete pass through the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics_df, x='epoch', y='loss', \n",
    "              title='Loss over Epochs', \n",
    "              labels={'epoch': 'Epoch', 'loss': 'Loss'},\n",
    "              markers=True)  # Enable markers on the line\n",
    "\n",
    "# Update layout for more customization\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch',  # Title for the X-axis\n",
    "    yaxis_title='Loss',   # Title for the Y-axis\n",
    "    font=dict(family=\"Courier New, monospace\", size=12, color=\"RebeccaPurple\"),\n",
    "    xaxis=dict(tickmode='auto', nticks=20),  # Auto mode for ticks, adjust as needed\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics_df, x='epoch', y='grad_norm', \n",
    "              title='Trend of Gradient Norms Across Epochs', \n",
    "              labels={'epoch': 'Epoch', 'grad_norm': 'Gradient Norm'},\n",
    "              markers=True)  # Enable markers on the line\n",
    "\n",
    "# Update layout for more customization\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch',  # Title for the X-axis\n",
    "    yaxis_title='Gradient Norm',   # Title for the Y-axis\n",
    "    font=dict(family=\"Courier New, monospace\", size=12, color=\"RebeccaPurple\"),\n",
    "    xaxis=dict(tickmode='auto', nticks=20),  # Auto mode for ticks, adjust as needed\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A typical and healthy training process is indicated by a decreasing gradient norm.\n",
    "- Stability in the gradient norm across epochs, without sudden spikes, indicates consistent learning. If the training process is stable, the gradient norms should not fluctuate excessively.\n",
    "- If the gradient norm becomes very small and does not change much, it could either mean the model has converged or is stuck in a plateau where it is not learning effectively.\n",
    "- On the other hand, a very small gradient norm, especially early in training, might be indicative of \"vanishing gradients.\" This is a situation where gradients become so small that they do not contribute effectively to updating weights, slowing down the training or stopping it prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics_df, x='epoch', y='learning_rate', \n",
    "              title='Trend of Learning Rate Across Epochs', \n",
    "              labels={'epoch': 'Epoch', 'learning_rate': 'Learning Rate'},\n",
    "              markers=True)  # Enable markers on the line\n",
    "\n",
    "# Update layout for more customization\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch',  # Title for the X-axis\n",
    "    yaxis_title='Learning Rate',   # Title for the Y-axis\n",
    "    font=dict(family=\"Courier New, monospace\", size=12, color=\"RebeccaPurple\"),\n",
    "    xaxis=dict(tickmode='auto', nticks=20),  # Auto mode for ticks, adjust as needed\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = './results/Roberta-Squad2-Subjqa'\n",
    "trainer.save_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
