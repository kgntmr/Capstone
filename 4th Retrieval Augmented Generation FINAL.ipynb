{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "kf0WMTIt7_XW"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain langchain-chroma langchain-openai chroma langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MolCS8DpD4rY",
        "outputId": "245d365e-8a31-4675-894c-e3e2ec895790"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import getpass\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the question-answering pipeline from the Fine-Tuned model\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"kgntmr/RoBERTa-SQuAD2.0-SubjQA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW8sy8AND4rZ",
        "outputId": "b1adf0e8-aa8a-4e0a-e05c-80073dfdf8d0"
      },
      "outputs": [],
      "source": [
        "# Get API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aDa0vwBLD4rZ"
      },
      "outputs": [],
      "source": [
        "# Define the WebBaseLoader class\n",
        "class WebBaseLoader:\n",
        "    # Constructor to initialize the WebBaseLoader object with web_paths and bs_kwargs\n",
        "    def __init__(self, web_paths, bs_kwargs):\n",
        "        self.web_paths = web_paths  # Stores a list of URLs to be processed\n",
        "        self.bs_kwargs = bs_kwargs  # Stores additional arguments for BeautifulSoup\n",
        "\n",
        "    # Method to load data from each web path and parse the HTML content\n",
        "    def load(self):\n",
        "        results = {}  # Dictionary to store the results of web scraping\n",
        "        for url in self.web_paths:  # Iterating over each URL in the web_paths list\n",
        "            try:\n",
        "                response = requests.get(url)  # Sending a GET request to the URL\n",
        "                if response.status_code == 200:  # Checking if the request was successful\n",
        "                    # Parsing the HTML content with BeautifulSoup using the provided arguments\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser', **self.bs_kwargs)\n",
        "                    results[url] = soup.get_text()  # Extracting text from the parsed HTML and storing it in the results dictionary\n",
        "                else:\n",
        "                    results[url] = None  # Storing None if the response was unsuccessful\n",
        "            except requests.RequestException as e:  # Handling exceptions that may occur during the GET request\n",
        "                results[url] = str(e)  # Storing the exception message as the result for the URL\n",
        "        return results  # Returning the dictionary containing the results of the web scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gOPHTrGwD4rZ"
      },
      "outputs": [],
      "source": [
        "# Function definition to fetch and return text content from specified website URLs using a given set of selector attributes\n",
        "def fetch_website_text(urls, selector_attrs):\n",
        "    # Creating a SoupStrainer object that filters out all unnecessary data except for elements matching the provided attributes\n",
        "    strainer = bs4.SoupStrainer(**selector_attrs)\n",
        "    # Initializing the WebBaseLoader with the URLs and the strainer object to parse only necessary parts of HTML\n",
        "    loader = WebBaseLoader(web_paths=urls, bs_kwargs={\"parse_only\": strainer})\n",
        "    # Calling the 'load' method from the WebBaseLoader instance to fetch and parse the web pages\n",
        "    return loader.load()\n",
        "\n",
        "# List of URLs from which to scrape data\n",
        "urls = [\n",
        "    \"https://www.theguardian.com/technology/2016/may/03/amazon-fresh-food-deliveries-understood-to-start-this-month\",\n",
        "    \"https://www.theguardian.com/media/2016/may/16/bbc-netflix-rival-itv-nbc-universal\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/28/amazon-most-profitable-quarter-sales-up-costs\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/26/amazon-kindle-oasis-review-luxury-e-reader\",\n",
        "    \"https://www.theguardian.com/environment/andes-to-the-amazon/2016/may/25/london-stock-exchange-amazon-deforestation\",\n",
        "    \"https://www.theguardian.com/media/2016/may/25/netflix-and-amazon-must-guarantee-20-of-content-is-european\",\n",
        "    \"https://www.theguardian.com/technology/2016/may/26/amazon-echo-virtual-assistant-child-privacy-law\",\n",
        "]\n",
        "# Dictionary specifying the attributes to filter HTML elements using SoupStrainer\n",
        "selector_attrs = {\"class\": \"article-body-commercial-selector\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U066izP1Sy5q"
      },
      "source": [
        "### The function fetch_website_text is now ready to be called with the list of URLs and selector attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dhkRHILWD4ra"
      },
      "outputs": [],
      "source": [
        "# Initialize the WebBaseLoader with URLs and BeautifulSoup keyword arguments\n",
        "loader = WebBaseLoader(urls, {\"parse_only\": bs4.SoupStrainer(**selector_attrs)})\n",
        "\n",
        "# Load the content from the specified URLs\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DGu7hfy2D4ra"
      },
      "outputs": [],
      "source": [
        "# Definition of the RecursiveCharacterTextSplitter class\n",
        "class RecursiveCharacterTextSplitter:\n",
        "    def __init__(self, chunk_size, chunk_overlap):\n",
        "        self.chunk_size = chunk_size  # The number of characters in each text chunk\n",
        "        self.chunk_overlap = chunk_overlap  # The number of characters each chunk overlaps with the next\n",
        "\n",
        "    def split_document(self, text):\n",
        "        return [text[i:i + self.chunk_size] for i in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
        "\n",
        "    def split_documents(self, documents):\n",
        "        splits = []  # List to hold all chunks from all documents\n",
        "        for doc in documents:  # Iterating over each document in the provided list\n",
        "            if isinstance(doc, str):\n",
        "                text = doc  # Directly assigns the document to text if it is a string\n",
        "            else:\n",
        "                text = getattr(doc, 'page_content', '')  # Attempts to fetch 'page_content' from the document object; defaults to empty string if not found\n",
        "            splits.extend(self.split_document(text))  # Adds the chunks from the current document to the splits list\n",
        "        return splits  # Returns the list of all chunks from all documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dDJtS21YTF6m"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of RecursiveCharacterTextSplitter with a chunk size of 1000 characters and an overlap of 200 characters\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "# Splitting a list of documents into smaller, overlapping chunks to maintain context between sections\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Initializing a vector store to enable semantic search capabilities using embeddings from OpenAI\n",
        "vectorstore = Chroma.from_texts(texts=splits, embedding=OpenAIEmbeddings())\n",
        "# Creating a retriever from the vector store for efficient information retrieval\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Retrieving a pre-defined prompt designed for use with language models in a retrieval-augmented generation setup\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to format strings from a list of documents into a single string\n",
        "def format_strings(documents):\n",
        "    formatted_documents = []  # List to hold formatted documents\n",
        "    for doc in documents:  # Iterate through each document in the input list\n",
        "        if isinstance(doc, str):\n",
        "            formatted_documents.append(doc)  # Add the string directly if the document is a string\n",
        "        elif isinstance(doc, dict):\n",
        "            # If the document is a dictionary, retrieve the value of 'page_content', defaulting to an empty string if not found\n",
        "            formatted_documents.append(doc.get('page_content', ''))\n",
        "        else:\n",
        "            # Append an empty string if the document is neither a string nor a dictionary\n",
        "            formatted_documents.append('')\n",
        "    # Join all formatted documents into a single string, separated by two newlines\n",
        "    return \"\\n\\n\".join(formatted_documents)\n",
        "\n",
        "# Usage of the function to format a list of documents\n",
        "formatted_context = format_strings(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the User Interface for RAG based on the user's queries\n",
        "def user_interface():\n",
        "    # User input for the question\n",
        "    question = input(\"Please enter your question about Amazon Apr - Jun 2016: \")\n",
        "    # Process the user's question using the retrieval-augmented generation pipeline\n",
        "    response = rag_answer(question, formatted_context)\n",
        "    print(\"Answer:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: most-profitable-quarter\n"
          ]
        }
      ],
      "source": [
        "# Function to generate answers using the RAG pipeline and provided context\n",
        "def rag_answer(question, context):\n",
        "    # Generate answer using the RAG pipeline\n",
        "    answer = qa_pipeline(question=question, context=context)\n",
        "    return answer['answer']  # Return only the 'answer' part of the result\n",
        "\n",
        "# Main function to handle the application's execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    user_interface()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
